{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1fa6176db50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize Data and Analysis Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Pytorch to train data\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data To Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/Reviews.csv',  \n",
    "                 low_memory=False)\n",
    "df = df[[\"Id\", \"Score\", \"Summary\"]]\n",
    "df = df.rename(columns={'Summary': 'Text'})\n",
    "# reset index\n",
    "# df.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Score                   Text\n",
       "0   1      5  Good Quality Dog Food\n",
       "1   2      1      Not as Advertised\n",
       "2   3      4  \"Delight\" says it all\n",
       "3   4      2         Cough Medicine\n",
       "4   5      5            Great taffy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thông tin DataFrame:\n",
    "\n",
    "- **Chỉ số (Index):** Range từ 0 đến 568453 (tổng cộng 568454 dòng).\n",
    "- **Số cột:** 3 cột (\"Id\", \"Score\", \"Text\").\n",
    "- **Kiểu dữ liệu cột:**\n",
    "  - \"Id\" và \"Score\": int64.\n",
    "  - \"Text\": object (chuỗi hoặc đối tượng không phải số).\n",
    "- **Giá trị không phải null:**\n",
    "  - Mỗi cột có 568454 giá trị không phải null.\n",
    "- **Dung lượng bộ nhớ:** Khoảng 13.0 MB.\n",
    "\n",
    "> 568454 - 568427 = 27 giá trị null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Id      568454 non-null  int64 \n",
      " 1   Score   568454 non-null  int64 \n",
      " 2   Text    568427 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 13.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Info Data:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df`: Đây là tên của DataFrame, giả sử đã được định nghĩa trước đó trong mã.\n",
    "\n",
    "- `df.Text`: Lấy cột có tên \"Text\" từ DataFrame `df`.\n",
    "\n",
    "- `.iloc[10]`: Lấy giá trị ở dòng thứ 10 của cột \"Text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Best Hot Sauce in the World'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null:\n",
    "df.Text.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id        0\n",
       "Score     0\n",
       "Text     27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xử lý xong các giá trị null, nếu có"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 568427 entries, 0 to 568453\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Id      568427 non-null  int64 \n",
      " 1   Score   568427 non-null  int64 \n",
      " 2   Text    568427 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 17.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# delete null values\n",
    "# Xóa các dòng có giá trị null\n",
    "df = df.dropna(subset=['Text'])\n",
    "# check for null\n",
    "df.isnull().sum()\n",
    "# Info Data:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "5    363122\n",
       "4     80655\n",
       "1     52268\n",
       "3     42638\n",
       "2     29744\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 1].index[1:50000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 2].index[1:27000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 3].index[1:40000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 4].index[1:77000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 5].index[1:360000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# df.Score.value_counts()\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lấy Series từ df['Score'].value_counts()\n",
    "score_counts = df['Score'].value_counts()\n",
    "\n",
    "# Thiết lập môi trường trực quan\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "# Vẽ biểu đồ thanh\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=score_counts.index, y=score_counts.values,\n",
    "            palette=\"Spectral\", legend=False, hue=score_counts.values)\n",
    "plt.title('Số lượng điểm (Score)')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Số lượng')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  128\n"
     ]
    }
   ],
   "source": [
    "# find the maximum length\n",
    "max_len = max([len(text) for text in df.Text])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spliting into Train, Test, Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **`from sklearn.model_selection import train_test_split`**: \n",
    "   - Nhập hàm `train_test_split` từ scikit-learn để thực hiện chia dữ liệu thành tập huấn luyện và tập kiểm tra.\n",
    "\n",
    "2. **`X_train, X_val, y_train, y_val = train_test_split(df.index.values, df.Score.values, test_size=0.15, random_state=17, stratify=df.Score.values)`**:\n",
    "   - `df.index.values`: Chọn cột chỉ mục của DataFrame (`index`), giả sử rằng nó chứa các giá trị duy nhất hoặc độc lập.\n",
    "   - `df.Score.values`: Chọn cột \"Score\" làm giá trị mục tiêu.\n",
    "   - `test_size=0.15`: Thiết lập tỷ lệ tập kiểm tra là 15%, tỷ lệ tập huấn luyện là 85%.\n",
    "   - `random_state=17`: Đặt một giá trị ngẫu nhiên để đảm bảo tái tạo kết quả nếu bạn muốn chạy lại mã và nhận được kết quả giống nhau.\n",
    "   - `stratify=df.Score.values`: Thiết lập để đảm bảo phân phối của tập kiểm tra giữ nguyên tỷ lệ của các lớp (stratified sampling), đặc biệt quan trọng nếu dữ liệu không cân bằng theo các giá trị của \"Score\".\n",
    "\n",
    "3. **`X_train, X_val, y_train, y_val`**:\n",
    "   - `X_train`, `X_val`: Chứa các chỉ mục (index) của dữ liệu tương ứng trong tập huấn luyện và tập kiểm tra.\n",
    "   - `y_train`, `y_val`: Chứa các giá trị \"Score\" tương ứng với tập huấn luyện và tập kiểm tra.\n",
    "\n",
    "Tổng cộng, đoạn mã này chia dữ liệu thành tập huấn luyện và tập kiểm tra, sử dụng 85% dữ liệu cho tập huấn luyện và 15% cho tập kiểm tra, và giữ nguyên tỷ lệ của các lớp trong quá trình chia dữ liệu. Điều này làm cho mô hình có thể học từ một phân phối dữ liệu tương tự như dữ liệu gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Score                   Text data_type\n",
       "0   1      5  Good Quality Dog Food   not_set\n",
       "1   2      1      Not as Advertised   not_set\n",
       "2   3      4  \"Delight\" says it all   not_set\n",
       "3   4      2         Cough Medicine   not_set\n",
       "4   5      5            Great taffy   not_set"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
    "                                                  df.Score.values,\n",
    "                                                  test_size=0.15,\n",
    "                                                  random_state=17,\n",
    "                                                  stratify=df.Score.values)\n",
    "# create new column\n",
    "df['data_type'] = ['not_set'] * df.shape[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in data type\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>44428</td>\n",
       "      <td>44428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>7840</td>\n",
       "      <td>7840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>25282</td>\n",
       "      <td>25282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>4462</td>\n",
       "      <td>4462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>36242</td>\n",
       "      <td>36242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>6396</td>\n",
       "      <td>6396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>68557</td>\n",
       "      <td>68557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>12098</td>\n",
       "      <td>12098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>train</th>\n",
       "      <td>308653</td>\n",
       "      <td>308653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>54469</td>\n",
       "      <td>54469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id    Text\n",
       "Score data_type                \n",
       "1     train       44428   44428\n",
       "      val          7840    7840\n",
       "2     train       25282   25282\n",
       "      val          4462    4462\n",
       "3     train       36242   36242\n",
       "      val          6396    6396\n",
       "4     train       68557   68557\n",
       "      val         12098   12098\n",
       "5     train      308653  308653\n",
       "      val         54469   54469"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Score','data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **`from transformers import BertTokenizer`**:\n",
    "   - `BertTokenizer` là một lớp từ thư viện Transformers của Hugging Face, được thiết kế để chuyển đổi văn bản thành đầu vào mà mô hình BERT có thể hiểu được. Nó cung cấp các phương thức để mã hóa văn bản thành các token và thêm các thông tin đặc biệt như token đặc biệt `[CLS]` và `[SEP]` sử dụng trong mô hình BERT.\n",
    "\n",
    "2. **`from torch.utils.data import TensorDataset`**:\n",
    "   - `TensorDataset` là một lớp từ thư viện torch, được sử dụng để tạo dataset cho PyTorch. Đây là một cách thuận tiện để tổ chức dữ liệu và truyền nó vào mô hình PyTorch. Lớp này chấp nhận một hoặc nhiều tensors và tạo ra một dataset với khả năng lập chỉ mục dữ liệu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_text = max(len(text) for text in df.Text)\n",
    "max_len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize train set\n",
    "encoded_data_train = tokenizer.batch_encode_plus(df[df.data_type == 'train'].Text.values,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 return_attention_mask=True,\n",
    "                                                 pad_to_max_length=True,\n",
    "                                                 max_length=max_len_text+1,\n",
    "                                                 return_tensors='pt',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer val set\n",
    "encoded_data_val = tokenizer.batch_encode_plus(df[df.data_type == 'val'].Text.values,\n",
    "                                               # add_special_tokens = True,\n",
    "                                               return_attention_mask=True,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_len_text+1,\n",
    "                                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2204,  3737,  ...,     0,     0,     0],\n",
       "        [  101,  2025,  2004,  ...,     0,     0,     0],\n",
       "        [  101,  1000, 12208,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  3819,  2005,  ...,     0,     0,     0],\n",
       "        [  101,  5440,  2731,  ...,     0,     0,     0],\n",
       "        [  101,  2307,  6861,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type == 'train'].Score.values, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode val set\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "\n",
    "# convert data type to torch.tensor\n",
    "labels_val = torch.tensor(df[df.data_type == 'val'].Score.values, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2204,  3737,  ...,     0,     0,     0],\n",
       "        [  101,  2025,  2004,  ...,     0,     0,     0],\n",
       "        [  101,  1000, 12208,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  3819,  2005,  ...,     0,     0,     0],\n",
       "        [  101,  5440,  2731,  ...,     0,     0,     0],\n",
       "        [  101,  2307,  6861,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 1, 4,  ..., 5, 5, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "dataset_train = TensorDataset(input_ids_train,\n",
    "                              attention_masks_train,\n",
    "                              labels_train)\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val,\n",
    "                            attention_masks_val,\n",
    "                            labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483162\n",
      "85265\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<torch.utils.data.dataset.TensorDataset object at 0x000001FA6B20E490>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2204,  3737,  ...,     0,     0,     0],\n",
       "         [  101,  2025,  2004,  ...,     0,     0,     0],\n",
       "         [  101,  1000, 12208,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  3819,  2005,  ...,     0,     0,     0],\n",
       "         [  101,  5440,  2731,  ...,     0,     0,     0],\n",
       "         [  101,  2307,  6861,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([5, 1, 4,  ..., 5, 5, 5]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting up BERT Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(df.Score.unique())+1,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.36.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# We Need two different dataloder\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              sampler=RandomSampler(dataset_train),\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=8,\n",
    "                                pin_memory=True\n",
    "\n",
    "                              )\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val,\n",
    "                                   sampler=RandomSampler(dataset_val),\n",
    "                                   batch_size=batch_size,\n",
    "                                   num_workers=8,\n",
    "                                   pin_memory=True\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15099"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setting Up Optimiser and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Defining our Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "\n",
    "    # Setting up the preds to axis=1\n",
    "    # Flatting it to a single iterable list of array\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "\n",
    "    # Flattening the labels\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Returning the f1_score as define by sklearn\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={'angry': 2,\n",
    " 'ish': 3,\n",
    " 'sad': 1,\n",
    " 'happy': 4,\n",
    " 'so happy': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Iterating over all the unique labels\n",
    "    # label_flat are the --> True labels\n",
    "    for label in np.unique(labels_flat):\n",
    "        # Taking out all the pred_flat where the True alable is the lable we care about.\n",
    "        # e.g. for the label Happy -- we Takes all Prediction for true happy flag\n",
    "        y_preds = preds_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds == label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a training loop to control PyTorch finetuning of BERT using CPU or GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in tqdm(dataloader_val):\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | model   | BertForSequenceClassification | 109 M \n",
      "1 | loss_fn | CrossEntropyLoss              | 0     \n",
      "----------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.947   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] Train Loss:  1.7206271886825562\n",
      "Epoch 0:   3%|▎         | 1/32 [00:32<16:41,  0.03it/s, v_num=10]Train Loss:  1.678783893585205\n",
      "Epoch 0:   6%|▋         | 2/32 [00:59<14:53,  0.03it/s, v_num=10]Train Loss:  1.5742733478546143\n",
      "Epoch 0:   9%|▉         | 3/32 [01:24<13:32,  0.04it/s, v_num=10]Train Loss:  1.6002278327941895\n",
      "Epoch 0:  12%|█▎        | 4/32 [01:49<12:44,  0.04it/s, v_num=10]Train Loss:  1.569271206855774\n",
      "Epoch 0:  16%|█▌        | 5/32 [02:13<11:58,  0.04it/s, v_num=10]Train Loss:  1.5151326656341553\n",
      "Epoch 0:  19%|█▉        | 6/32 [02:37<11:23,  0.04it/s, v_num=10]Train Loss:  1.5735825300216675\n",
      "Epoch 0:  22%|██▏       | 7/32 [03:01<10:46,  0.04it/s, v_num=10]Train Loss:  1.461680293083191\n",
      "Epoch 0:  25%|██▌       | 8/32 [03:24<10:13,  0.04it/s, v_num=10]Train Loss:  1.5737792253494263\n",
      "Epoch 0:  28%|██▊       | 9/32 [03:47<09:42,  0.04it/s, v_num=10]Train Loss:  1.4059066772460938\n",
      "Epoch 0:  31%|███▏      | 10/32 [04:10<09:11,  0.04it/s, v_num=10]Train Loss:  1.454366683959961\n",
      "Epoch 0:  34%|███▍      | 11/32 [04:33<08:41,  0.04it/s, v_num=10]Train Loss:  1.4885820150375366\n",
      "Epoch 0:  38%|███▊      | 12/32 [04:56<08:14,  0.04it/s, v_num=10]Train Loss:  1.4400323629379272\n",
      "Epoch 0:  41%|████      | 13/32 [05:19<07:46,  0.04it/s, v_num=10]Train Loss:  1.5593008995056152\n",
      "Epoch 0:  44%|████▍     | 14/32 [05:41<07:19,  0.04it/s, v_num=10]Train Loss:  1.4163451194763184\n",
      "Epoch 0:  47%|████▋     | 15/32 [06:04<06:53,  0.04it/s, v_num=10]Train Loss:  1.2494561672210693\n",
      "Epoch 0:  50%|█████     | 16/32 [06:27<06:27,  0.04it/s, v_num=10]Train Loss:  1.3929600715637207\n",
      "Epoch 0:  53%|█████▎    | 17/32 [06:50<06:02,  0.04it/s, v_num=10]Train Loss:  1.1566112041473389\n",
      "Epoch 0:  56%|█████▋    | 18/32 [07:14<05:38,  0.04it/s, v_num=10]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "class PretrainedBert(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, scheduler, dataloader_train, dataloader_validation, label_dict):\n",
    "        super(PretrainedBert, self).__init__()\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_validation = dataloader_validation\n",
    "        self.label_dict = label_dict\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        # Initialize GradScaler for mixed-precision training\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        with autocast():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        # Backward pass with manual gradient scaling\n",
    "        scaled_loss = loss / self.gradient_accumulation_steps\n",
    "        scaled_loss.backward(retain_graph=True)\n",
    "\n",
    "        if (batch_idx + 1) % self.gradient_accumulation_steps:\n",
    "            # Optimization step\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss.item(), on_step=True, on_epoch=True)\n",
    "        print(\"Train Loss: \", loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        with autocast():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # Log validation loss\n",
    "        self.log('val_loss', loss.item(), on_step=True, on_epoch=True)\n",
    "        # print(\"Val Loss: \", loss.item())\n",
    "\n",
    "        # Additional metrics, e.g., F1 score and accuracy\n",
    "        predictions = outputs.logits.argmax().squeeze()\n",
    "        true_vals = labels\n",
    "\n",
    "        # Additional check for dimensions\n",
    "        if len(predictions.shape) != len(true_vals.shape):\n",
    "            pass\n",
    "            # print(\"Mismatch in dimensions - predictions and true_vals\")\n",
    "            # Handle this discrepancy according to your model's requirements\n",
    "        else:\n",
    "            val_f1 = self.compute_f1_score(predictions.cpu().numpy(), true_vals.cpu().numpy())\n",
    "            val_acc = self.compute_accuracy(predictions.cpu().numpy(), true_vals.cpu().numpy())\n",
    "\n",
    "            # Log F1 score and accuracy\n",
    "            self.log('val_f1', val_f1, on_step=False, on_epoch=True)\n",
    "            # print(\"Val F1: \", val_acc)\n",
    "\n",
    "            self.log('val_acc', val_acc, on_step=False, on_epoch=True)\n",
    "            # print(\"Val Acc: \", val_acc)\n",
    "\n",
    "\n",
    "    def compute_f1_score(self, predictions, labels):\n",
    "        f1 = f1_score(labels, predictions, average='weighted')\n",
    "        return f1\n",
    "\n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        return accuracy\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer], [self.scheduler]\n",
    "# Create Lightning model\n",
    "lightning_model = PretrainedBert(model, optimizer, scheduler, dataloader_train, dataloader_validation, label_dict)\n",
    "\n",
    "# Create PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    limit_train_batches=batch_size,\n",
    ")\n",
    "# Train the model\n",
    "trainer.fit(lightning_model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_validation)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
