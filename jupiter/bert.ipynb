{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1167e9defd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize Data and Analysis Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Pytorch to train data\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data To Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good and interesting</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107013</th>\n",
       "      <td>107013</td>\n",
       "      <td>Trendy topic with talks from expertises in the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107014</th>\n",
       "      <td>107014</td>\n",
       "      <td>Wonderful! Simple and clear language, good ins...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107015</th>\n",
       "      <td>107015</td>\n",
       "      <td>an interesting and fun course. thanks. dr quincy</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107016</th>\n",
       "      <td>107016</td>\n",
       "      <td>very broad perspective, up to date information...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107017</th>\n",
       "      <td>107017</td>\n",
       "      <td>An informative course on the social and financ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107018 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                             Review  Label\n",
       "0            0                               good and interesting      5\n",
       "1            1  This class is very helpful to me. Currently, I...      5\n",
       "2            2  like!Prof and TAs are helpful and the discussi...      5\n",
       "3            3  Easy to follow and includes a lot basic and im...      5\n",
       "4            4  Really nice teacher!I could got the point eazl...      4\n",
       "...        ...                                                ...    ...\n",
       "107013  107013  Trendy topic with talks from expertises in the...      4\n",
       "107014  107014  Wonderful! Simple and clear language, good ins...      5\n",
       "107015  107015   an interesting and fun course. thanks. dr quincy      5\n",
       "107016  107016  very broad perspective, up to date information...      4\n",
       "107017  107017  An informative course on the social and financ...      4\n",
       "\n",
       "[107018 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/reviews.csv',  \n",
    "                 low_memory=False)\n",
    "df\n",
    "# reset index\n",
    "# df.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns to map to the original data\n",
    "# df=df.rename(columns={'reviews': 'Text', 'rating': 'Score'})\n",
    "# # Select only the columns we need\n",
    "# df = df[['Text', 'Score']]\n",
    "# # Add columns id for each score\n",
    "# _dict_score = {0: 'Very Bad', 1: 'Bad', 2: 'Neutral', 3: 'Good', 4: 'Very Good'}\n",
    "# _get_index = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n",
    "# # Add columns id for each score\n",
    "# df['Id'] = df['Score']\n",
    "# df['Id']=df['Id'].map(_get_index)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'Review': 'Text', 'Label': 'Score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good and interesting</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text  Score\n",
       "0   0                               good and interesting      5\n",
       "1   1  This class is very helpful to me. Currently, I...      5\n",
       "2   2  like!Prof and TAs are helpful and the discussi...      5\n",
       "3   3  Easy to follow and includes a lot basic and im...      5\n",
       "4   4  Really nice teacher!I could got the point eazl...      4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thông tin DataFrame:\n",
    "\n",
    "- **Chỉ số (Index):** Range từ 0 đến 568453 (tổng cộng 568454 dòng).\n",
    "- **Số cột:** 3 cột (\"Id\", \"Score\", \"Text\").\n",
    "- **Kiểu dữ liệu cột:**\n",
    "  - \"Id\" và \"Score\": int64.\n",
    "  - \"Text\": object (chuỗi hoặc đối tượng không phải số).\n",
    "- **Giá trị không phải null:**\n",
    "  - Mỗi cột có 568454 giá trị không phải null.\n",
    "- **Dung lượng bộ nhớ:** Khoảng 13.0 MB.\n",
    "\n",
    "> 568454 - 568427 = 27 giá trị null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lọc 512 token cho Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Text'].apply(lambda x: len(str(x)) < 512)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 102869 entries, 0 to 107017\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Id      102869 non-null  int64 \n",
      " 1   Text    102869 non-null  object\n",
      " 2   Score   102869 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Info Data:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df`: Đây là tên của DataFrame, giả sử đã được định nghĩa trước đó trong mã.\n",
    "\n",
    "- `df.Text`: Lấy cột có tên \"Text\" từ DataFrame `df`.\n",
    "\n",
    "- `.iloc[10]`: Lấy giá trị ở dòng thứ 10 của cột \"Text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Is there any reason why you should not apply the course by BCG?)It's content is pretty unique and includes a high level analysis and a wide range of knowledge needed to cover all detailed aspects.Best regards,Oleg Serov\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null:\n",
    "df.Text.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id       0\n",
       "Text     0\n",
       "Score    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xử lý xong các giá trị null, nếu có"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 102869 entries, 0 to 107017\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Id      102869 non-null  int64 \n",
      " 1   Text    102869 non-null  object\n",
      " 2   Score   102869 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# delete null values\n",
    "# Xóa các dòng có giá trị null\n",
    "df = df.dropna(subset=['Text'])\n",
    "# check for null\n",
    "df.isnull().sum()\n",
    "# Info Data:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "5    77203\n",
       "4    17192\n",
       "3     4548\n",
       "1     2065\n",
       "2     1861\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 1].index[1:40000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 2].index[1:20000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 3].index[1:30000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 4].index[1:70000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# # Reduce data:\n",
    "# indices_to_remove = df[df['Score'] == 5].index[1:350000]\n",
    "# df = df.drop(indices_to_remove)\n",
    "# df.Score.value_counts()\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lấy Series từ df['Score'].value_counts()\n",
    "score_counts = df['Score'].value_counts()\n",
    "\n",
    "# Thiết lập môi trường trực quan\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "# Vẽ biểu đồ thanh\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=score_counts.index, y=score_counts.values,\n",
    "            palette=\"Spectral\", legend=False, hue=score_counts.values)\n",
    "plt.title('Score Reviews')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  511\n"
     ]
    }
   ],
   "source": [
    "# find the maximum length\n",
    "max_len = max([len(text) for text in df.Text])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spliting into Train, Test, Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **`from sklearn.model_selection import train_test_split`**: \n",
    "   - Nhập hàm `train_test_split` từ scikit-learn để thực hiện chia dữ liệu thành tập huấn luyện và tập kiểm tra.\n",
    "\n",
    "2. **`X_train, X_val, y_train, y_val = train_test_split(df.index.values, df.Score.values, test_size=0.15, random_state=17, stratify=df.Score.values)`**:\n",
    "   - `df.index.values`: Chọn cột chỉ mục của DataFrame (`index`), giả sử rằng nó chứa các giá trị duy nhất hoặc độc lập.\n",
    "   - `df.Score.values`: Chọn cột \"Score\" làm giá trị mục tiêu.\n",
    "   - `test_size=0.15`: Thiết lập tỷ lệ tập kiểm tra là 15%, tỷ lệ tập huấn luyện là 85%.\n",
    "   - `random_state=17`: Đặt một giá trị ngẫu nhiên để đảm bảo tái tạo kết quả nếu bạn muốn chạy lại mã và nhận được kết quả giống nhau.\n",
    "   - `stratify=df.Score.values`: Thiết lập để đảm bảo phân phối của tập kiểm tra giữ nguyên tỷ lệ của các lớp (stratified sampling), đặc biệt quan trọng nếu dữ liệu không cân bằng theo các giá trị của \"Score\".\n",
    "\n",
    "3. **`X_train, X_val, y_train, y_val`**:\n",
    "   - `X_train`, `X_val`: Chứa các chỉ mục (index) của dữ liệu tương ứng trong tập huấn luyện và tập kiểm tra.\n",
    "   - `y_train`, `y_val`: Chứa các giá trị \"Score\" tương ứng với tập huấn luyện và tập kiểm tra.\n",
    "\n",
    "Tổng cộng, đoạn mã này chia dữ liệu thành tập huấn luyện và tập kiểm tra, sử dụng 85% dữ liệu cho tập huấn luyện và 15% cho tập kiểm tra, và giữ nguyên tỷ lệ của các lớp trong quá trình chia dữ liệu. Điều này làm cho mô hình có thể học từ một phân phối dữ liệu tương tự như dữ liệu gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good and interesting</td>\n",
       "      <td>5</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>5</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>5</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>5</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>4</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text  Score data_type\n",
       "0   0                               good and interesting      5   not_set\n",
       "1   1  This class is very helpful to me. Currently, I...      5   not_set\n",
       "2   2  like!Prof and TAs are helpful and the discussi...      5   not_set\n",
       "3   3  Easy to follow and includes a lot basic and im...      5   not_set\n",
       "4   4  Really nice teacher!I could got the point eazl...      4   not_set"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
    "                                                  df.Score.values,\n",
    "                                                  test_size=0.15,\n",
    "                                                  random_state=17,\n",
    "                                                  stratify=df.Score.values)\n",
    "# create new column\n",
    "df['data_type'] = ['not_set'] * df.shape[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in data type\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>1755</td>\n",
       "      <td>1755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>1582</td>\n",
       "      <td>1582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>3866</td>\n",
       "      <td>3866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>682</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>14613</td>\n",
       "      <td>14613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>2579</td>\n",
       "      <td>2579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>train</th>\n",
       "      <td>65622</td>\n",
       "      <td>65622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>11581</td>\n",
       "      <td>11581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id   Text\n",
       "Score data_type              \n",
       "1     train       1755   1755\n",
       "      val          310    310\n",
       "2     train       1582   1582\n",
       "      val          279    279\n",
       "3     train       3866   3866\n",
       "      val          682    682\n",
       "4     train      14613  14613\n",
       "      val         2579   2579\n",
       "5     train      65622  65622\n",
       "      val        11581  11581"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Score','data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **`from transformers import BertTokenizer`**:\n",
    "   - `BertTokenizer` là một lớp từ thư viện Transformers của Hugging Face, được thiết kế để chuyển đổi văn bản thành đầu vào mà mô hình BERT có thể hiểu được. Nó cung cấp các phương thức để mã hóa văn bản thành các token và thêm các thông tin đặc biệt như token đặc biệt `[CLS]` và `[SEP]` sử dụng trong mô hình BERT.\n",
    "\n",
    "2. **`from torch.utils.data import TensorDataset`**:\n",
    "   - `TensorDataset` là một lớp từ thư viện torch, được sử dụng để tạo dataset cho PyTorch. Đây là một cách thuận tiện để tổ chức dữ liệu và truyền nó vào mô hình PyTorch. Lớp này chấp nhận một hoặc nhiều tensors và tạo ra một dataset với khả năng lập chỉ mục dữ liệu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_text = max(len(text) for text in df.Text)\n",
    "max_len_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giải pháp cho text dài vì tokenize bert chỉ 512 token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mã hóa dữ liệu cho từng giới hạn của text\n",
    "def encode_long_sequences(texts, max_len_text, tokenizer, data_type):\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True if data_type=='train' else False,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_len_text+1,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return encoded_data\n",
    "\n",
    "def encode_texts(df, data_type, max_len_text, tokenizer):\n",
    "    texts = df[df.data_type == data_type].Text.values\n",
    "    encoded_data = []\n",
    "\n",
    "    max_segment_len = max_len_text  # Tối đa 512 tokens cho mỗi segment\n",
    "\n",
    "    for text in texts:\n",
    "        text_segments = [text[i:i + max_segment_len] for i in range(0, len(text), max_segment_len)]\n",
    "        encoded_segments = encode_long_sequences(text_segments, max_segment_len, tokenizer, data_type)\n",
    "        encoded_data.append(encoded_segments)\n",
    "\n",
    "    merged_data = {}\n",
    "    for key in encoded_data[0].keys():\n",
    "        merged_data[key] = torch.cat([item[key] for item in encoded_data], dim=0)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# encode train data\n",
    "encoded_train_data = encode_texts(df, 'train', 510, tokenizer) # 510 tokens cho mỗi segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # tokenize train set\n",
    "# encoded_train_data = tokenizer.batch_encode_plus(df[df.data_type == 'train'].Text.values,\n",
    "#                                                  add_special_tokens=True,\n",
    "#                                                  return_attention_mask=True,\n",
    "#                                                  pad_to_max_length=True,\n",
    "#                                                  max_length=max_len_text+1,\n",
    "#                                                  return_tensors='pt',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer val set\n",
    "# encoded_data_val = tokenizer.batch_encode_plus(df[df.data_type == 'val'].Text.values,\n",
    "#                                                # add_special_tokens = True,\n",
    "#                                                return_attention_mask=True,\n",
    "#                                                pad_to_max_length=True,\n",
    "#                                                max_length=max_len_text+1,\n",
    "#                                                return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer val set\n",
    "encoded_val_data = encode_texts(df, 'val', 510, tokenizer) # 510 tokens cho mỗi segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2204,  1998,  ...,     0,     0,     0],\n",
       "         [  101,  2023,  2465,  ...,     0,     0,     0],\n",
       "         [  101,  2066,   999,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2019,  5875,  ...,     0,     0,     0],\n",
       "         [  101,  2200,  5041,  ...,     0,     0,     0],\n",
       "         [  101,  2019, 12367,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode train set\n",
    "input_ids_train = encoded_train_data['input_ids']\n",
    "attention_masks_train = encoded_train_data['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type == 'train'].Score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\jupiter\\bert.ipynb Cell 39\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# encode val set\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m input_ids_val \u001b[39m=\u001b[39m encoded_val_data[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m attention_masks_val \u001b[39m=\u001b[39m encoded_val_data[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# convert data type to torch.tensor\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoded_val_data' is not defined"
     ]
    }
   ],
   "source": [
    "# encode val set\n",
    "input_ids_val = encoded_val_data['input_ids']\n",
    "attention_masks_val = encoded_val_data['attention_mask']\n",
    "\n",
    "# convert data type to torch.tensor\n",
    "labels_val = torch.tensor(df[df.data_type == 'val'].Score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87459, 511])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87459, 511])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87438])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\jupiter\\bert.ipynb Cell 43\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create dataloader\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset_train \u001b[39m=\u001b[39m TensorDataset(input_ids_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                               attention_masks_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                               labels_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dataset_val \u001b[39m=\u001b[39m TensorDataset(input_ids_val,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                             attention_masks_val,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                             labels_val)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\utils\\data\\dataset.py:204\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[1;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "dataset_train = TensorDataset(input_ids_train,\n",
    "                              attention_masks_train,\n",
    "                              labels_train)\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val,\n",
    "                            attention_masks_val,\n",
    "                            labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87438\n",
      "15431\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<torch.utils.data.dataset.TensorDataset object at 0x000001CC08599790>'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2204,  1998,  ...,     0,     0,     0],\n",
       "         [  101,  2023,  2465,  ...,     0,     0,     0],\n",
       "         [  101,  2066,   999,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2019,  5875,  ...,     0,     0,     0],\n",
       "         [  101,  2200,  5041,  ...,     0,     0,     0],\n",
       "         [  101,  2019, 12367,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([5, 5, 5,  ..., 5, 4, 4]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting up BERT Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(df.Score.unique()),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# We Need two different dataloder\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              sampler=RandomSampler(dataset_train),\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=1,\n",
    "                                pin_memory=True\n",
    "\n",
    "                              )\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val,\n",
    "                                   sampler=RandomSampler(dataset_val),\n",
    "                                   batch_size=batch_size,\n",
    "                                   num_workers=1,\n",
    "                                   pin_memory=True\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setting Up Optimiser and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Defining our Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "\n",
    "    # Setting up the preds to axis=1\n",
    "    # Flatting it to a single iterable list of array\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "\n",
    "    # Flattening the labels\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Returning the f1_score as define by sklearn\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 'Very Bad', 1: 'Bad', 2: 'Neutral', 3: 'Good', 4: 'Very Good'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Iterating over all the unique labels\n",
    "    # label_flat are the --> True labels\n",
    "    for label in np.unique(labels_flat):\n",
    "        # Taking out all the pred_flat where the True alable is the lable we care about.\n",
    "        # e.g. for the label Happy -- we Takes all Prediction for true happy flag\n",
    "        y_preds = preds_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds == label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a training loop to control PyTorch finetuning of BERT using CPU or GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 69.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\jupiter\\bert.ipynb Cell 66\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y122sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y122sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(device)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2267\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2268\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2269\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m         )\n\u001b[1;32m-> 2271\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 810 (3 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 69.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val, model=model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in tqdm(dataloader_val):\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messure step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3fb9aa0b9a426a86d97c6fd7881c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 55.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\jupiter\\bert.ipynb Cell 69\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y125sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Define inputs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y125sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m0\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y125sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m1\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y125sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: batch[\u001b[39m2\u001b[39m]}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y125sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y125sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output.loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work_Space/ML/final_project/Mashine-Learning---RoBerta---Base-Bert/jupiter/bert.ipynb#Y125sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss_train_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1565\u001b[0m     input_ids,\n\u001b[0;32m   1566\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1567\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1568\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1569\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1570\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1574\u001b[0m )\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    608\u001b[0m         hidden_states,\n\u001b[0;32m    609\u001b[0m         attention_mask,\n\u001b[0;32m    610\u001b[0m         layer_head_mask,\n\u001b[0;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    613\u001b[0m         past_key_value,\n\u001b[0;32m    614\u001b[0m         output_attentions,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:359\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    355\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    357\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[0;32m    361\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Work_Space\\ML\\final_project\\Mashine-Learning---RoBerta---Base-Bert\\venv390\\lib\\site-packages\\torch\\nn\\functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1265\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1266\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 55.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    # Set model in train mode\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variable\n",
    "    loss_train_total = 0\n",
    "    print(f'Epoch {epoch}')\n",
    "    # set up progress bar\n",
    "    progress_bar = tqdm(dataloader_train,\n",
    "                        desc='Epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                        disable=False)\n",
    "    for batch in progress_bar:\n",
    "        # Set gradient to 0\n",
    "        model.zero_grad()\n",
    "        # Load into GPU\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        # Define inputs\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]  # output.loss\n",
    "        loss_train_total += loss.item()\n",
    "        # Backward pass to get gradients\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0 to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update optimizer\n",
    "        optimizer.step()\n",
    "        # Update scheduler\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f'Training loss: {loss.item()/len(batch)}')\n",
    "        # break\n",
    "\n",
    "\n",
    "\n",
    "    model_save_path = f'./model/pretrained_bert_model_{epoch}.pt'\n",
    "\n",
    "    torch.save(model, model_save_path)\n",
    "\n",
    "\n",
    "    # Print training result\n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "\n",
    "    print(f'Training loss: {loss_train_avg}')\n",
    "    # Evaluate\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    # F1 score\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    print(f'Validation loss: {val_loss}')\n",
    "    print(f'F1 Score (weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Đường dẫn đến tệp .pt đã lưu\n",
    "model_path = '../model/pretrained_bert_model_1.pt'\n",
    "\n",
    "# Khởi tạo mô hình BERT cho phân loại chuỗi và tải trạng thái từ tệp đã lưu\n",
    "load_model = torch.load(model_path)\n",
    "\n",
    "load_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb69021ee754adb899cea8980d0c77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#evaluate\n",
    "_, predictions, true_vals = evaluate(dataloader_validation, load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: so sad\n",
      "Accuracy: 212/340\n",
      "\n",
      "Class: sad\n",
      "Accuracy: 82/337\n",
      "\n",
      "Class: worry\n",
      "Accuracy: 195/396\n",
      "\n",
      "Class: happy\n",
      "Accuracy: 216/398\n",
      "\n",
      "Class: so happy\n",
      "Accuracy: 195/319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get accuracy score\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
