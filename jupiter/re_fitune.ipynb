{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (BertTokenizer,AutoTokenizer, BertForSequenceClassification, \n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data To Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good and interesting</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107013</th>\n",
       "      <td>107013</td>\n",
       "      <td>Trendy topic with talks from expertises in the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107014</th>\n",
       "      <td>107014</td>\n",
       "      <td>Wonderful! Simple and clear language, good ins...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107015</th>\n",
       "      <td>107015</td>\n",
       "      <td>an interesting and fun course. thanks. dr quincy</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107016</th>\n",
       "      <td>107016</td>\n",
       "      <td>very broad perspective, up to date information...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107017</th>\n",
       "      <td>107017</td>\n",
       "      <td>An informative course on the social and financ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107018 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                             Review  Label\n",
       "0            0                               good and interesting      5\n",
       "1            1  This class is very helpful to me. Currently, I...      5\n",
       "2            2  like!Prof and TAs are helpful and the discussi...      5\n",
       "3            3  Easy to follow and includes a lot basic and im...      5\n",
       "4            4  Really nice teacher!I could got the point eazl...      4\n",
       "...        ...                                                ...    ...\n",
       "107013  107013  Trendy topic with talks from expertises in the...      4\n",
       "107014  107014  Wonderful! Simple and clear language, good ins...      5\n",
       "107015  107015   an interesting and fun course. thanks. dr quincy      5\n",
       "107016  107016  very broad perspective, up to date information...      4\n",
       "107017  107017  An informative course on the social and financ...      4\n",
       "\n",
       "[107018 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/reviews.csv',  \n",
    "                 low_memory=False)\n",
    "df\n",
    "# reset index\n",
    "# df.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good and interesting</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                             Review  Label\n",
       "0   0                               good and interesting      5\n",
       "1   1  This class is very helpful to me. Currently, I...      5\n",
       "2   2  like!Prof and TAs are helpful and the discussi...      5\n",
       "3   3  Easy to follow and includes a lot basic and im...      5\n",
       "4   4  Really nice teacher!I could got the point eazl...      4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thông tin DataFrame:\n",
    "\n",
    "- **Chỉ số (Index):** Range từ 0 đến 568453 (tổng cộng 568454 dòng).\n",
    "- **Số cột:** 3 cột (\"Id\", \"Score\", \"Text\").\n",
    "- **Kiểu dữ liệu cột:**\n",
    "  - \"Id\" và \"Score\": int64.\n",
    "  - \"Text\": object (chuỗi hoặc đối tượng không phải số).\n",
    "- **Giá trị không phải null:**\n",
    "  - Mỗi cột có 568454 giá trị không phải null.\n",
    "- **Dung lượng bộ nhớ:** Khoảng 13.0 MB.\n",
    "\n",
    "> 568454 - 568427 = 27 giá trị null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 107018 entries, 0 to 107017\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Id      107018 non-null  int64 \n",
      " 1   Review  107018 non-null  object\n",
      " 2   Label   107018 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Info Data:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df`: Đây là tên của DataFrame, giả sử đã được định nghĩa trước đó trong mã.\n",
    "\n",
    "- `df.Text`: Lấy cột có tên \"Text\" từ DataFrame `df`.\n",
    "\n",
    "- `.iloc[10]`: Lấy giá trị ở dòng thứ 10 của cột \"Text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id        0\n",
       "Review    0\n",
       "Label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xử lý xong các giá trị null, nếu có"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'Review': 'Text', 'Label': 'Score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 107018 entries, 0 to 107017\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Id      107018 non-null  int64 \n",
      " 1   Text    107018 non-null  object\n",
      " 2   Score   107018 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# delete null values\n",
    "# Xóa các dòng có giá trị null\n",
    "df = df.dropna(subset=['Text'])\n",
    "# check for null\n",
    "df.isnull().sum()\n",
    "# Info Data:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo ánh xạ từ điểm số sang nhãn cảm xúc\n",
    "score_to_sentiment = {\n",
    "    5: 'happy',\n",
    "    4: 'satisfied',\n",
    "    3: 'neutral',\n",
    "    2: 'disappointed',\n",
    "    1: 'angry'\n",
    "}\n",
    "\n",
    "# Thay đổi cột \"Score\" thành cột \"Sentiment\"\n",
    "df['Sentiment'] = df['Score'].map(score_to_sentiment)\n",
    "\n",
    "# Loại bỏ cột \"Score\" nếu không cần thiết\n",
    "df = df.drop(columns=['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lấy Series từ df['Score'].value_counts()\n",
    "score_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Thiết lập môi trường trực quan\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "# Vẽ biểu đồ thanh\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=score_counts.index, y=score_counts.values,\n",
    "            palette=\"Spectral\", legend=False, hue=score_counts.values)\n",
    "plt.title('Sentiment Reviews')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  7766\n"
     ]
    }
   ],
   "source": [
    "# find the maximum length\n",
    "max_len = max([len(text) for text in df.Text])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good and interesting</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text  Sentiment  Label\n",
       "0   0                               good and interesting      happy      0\n",
       "1   1  This class is very helpful to me. Currently, I...      happy      0\n",
       "2   2  like!Prof and TAs are helpful and the discussi...      happy      0\n",
       "3   3  Easy to follow and includes a lot basic and im...      happy      0\n",
       "4   4  Really nice teacher!I could got the point eazl...  satisfied      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### creating labels ###\n",
    "\n",
    "possible_labels = df.Sentiment.unique()\n",
    "\n",
    "## creating a dict to convert string categories to numbers ##\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "    \n",
    "df['Label'] = df.Sentiment.replace(label_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happy': 0, 'satisfied': 1, 'neutral': 2, 'disappointed': 3, 'angry': 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spliting into Train, Test, Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **`from sklearn.model_selection import train_test_split`**: \n",
    "   - Nhập hàm `train_test_split` từ scikit-learn để thực hiện chia dữ liệu thành tập huấn luyện và tập kiểm tra.\n",
    "\n",
    "2. **`X_train, X_val, y_train, y_val = train_test_split(df.index.values, df.Score.values, test_size=0.15, random_state=17, stratify=df.Score.values)`**:\n",
    "   - `df.index.values`: Chọn cột chỉ mục của DataFrame (`index`), giả sử rằng nó chứa các giá trị duy nhất hoặc độc lập.\n",
    "   - `df.Score.values`: Chọn cột \"Score\" làm giá trị mục tiêu.\n",
    "   - `test_size=0.15`: Thiết lập tỷ lệ tập kiểm tra là 15%, tỷ lệ tập huấn luyện là 85%.\n",
    "   - `random_state=17`: Đặt một giá trị ngẫu nhiên để đảm bảo tái tạo kết quả nếu bạn muốn chạy lại mã và nhận được kết quả giống nhau.\n",
    "   - `stratify=df.Score.values`: Thiết lập để đảm bảo phân phối của tập kiểm tra giữ nguyên tỷ lệ của các lớp (stratified sampling), đặc biệt quan trọng nếu dữ liệu không cân bằng theo các giá trị của \"Score\".\n",
    "\n",
    "3. **`X_train, X_val, y_train, y_val`**:\n",
    "   - `X_train`, `X_val`: Chứa các chỉ mục (index) của dữ liệu tương ứng trong tập huấn luyện và tập kiểm tra.\n",
    "   - `y_train`, `y_val`: Chứa các giá trị \"Score\" tương ứng với tập huấn luyện và tập kiểm tra.\n",
    "\n",
    "Tổng cộng, đoạn mã này chia dữ liệu thành tập huấn luyện và tập kiểm tra, sử dụng 85% dữ liệu cho tập huấn luyện và 15% cho tập kiểm tra, và giữ nguyên tỷ lệ của các lớp trong quá trình chia dữ liệu. Điều này làm cho mô hình có thể học từ một phân phối dữ liệu tương tự như dữ liệu gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good and interesting</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This class is very helpful to me. Currently, I...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>like!Prof and TAs are helpful and the discussi...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Easy to follow and includes a lot basic and im...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Really nice teacher!I could got the point eazl...</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>1</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text  Sentiment  Label  \\\n",
       "0   0                               good and interesting      happy      0   \n",
       "1   1  This class is very helpful to me. Currently, I...      happy      0   \n",
       "2   2  like!Prof and TAs are helpful and the discussi...      happy      0   \n",
       "3   3  Easy to follow and includes a lot basic and im...      happy      0   \n",
       "4   4  Really nice teacher!I could got the point eazl...  satisfied      1   \n",
       "\n",
       "  data_type  \n",
       "0   not_set  \n",
       "1   not_set  \n",
       "2   not_set  \n",
       "3   not_set  \n",
       "4   not_set  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
    "                                                  df.Sentiment.values,\n",
    "                                                  test_size=0.15,\n",
    "                                                  random_state=17,\n",
    "                                                  stratify=df.Sentiment.values)\n",
    "# create new column\n",
    "df['data_type'] = ['not_set'] * df.shape[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in data type\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">angry</th>\n",
       "      <th>train</th>\n",
       "      <td>2099</td>\n",
       "      <td>2099</td>\n",
       "      <td>2099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">disappointed</th>\n",
       "      <th>train</th>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">happy</th>\n",
       "      <th>train</th>\n",
       "      <td>67297</td>\n",
       "      <td>67297</td>\n",
       "      <td>67297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>11876</td>\n",
       "      <td>11876</td>\n",
       "      <td>11876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">neutral</th>\n",
       "      <th>train</th>\n",
       "      <td>4310</td>\n",
       "      <td>4310</td>\n",
       "      <td>4310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>761</td>\n",
       "      <td>761</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">satisfied</th>\n",
       "      <th>train</th>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>2708</td>\n",
       "      <td>2708</td>\n",
       "      <td>2708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Id   Text  Label\n",
       "Sentiment    data_type                     \n",
       "angry        train       2099   2099   2099\n",
       "             val          370    370    370\n",
       "disappointed train       1913   1913   1913\n",
       "             val          338    338    338\n",
       "happy        train      67297  67297  67297\n",
       "             val        11876  11876  11876\n",
       "neutral      train       4310   4310   4310\n",
       "             val          761    761    761\n",
       "satisfied    train      15346  15346  15346\n",
       "             val         2708   2708   2708"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Sentiment','data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "d:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Using the BERT tokenizer from the 'bert-base-uncased' model\n",
    "# and setting do_lower_case to True to ensure all text is lowercased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "# Encoding the text data in the training set using batch_encode_plus\n",
    "# This method tokenizes and encodes a batch of sequences, adding special tokens,\n",
    "# padding the sequences to the same length, and returning PyTorch tensors\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].Text.values,            # Extracting text data for training\n",
    "    add_special_tokens=True,                          # Adding special tokens like [CLS] and [SEP]\n",
    "    return_attention_mask=True,                      # Returning attention masks to focus on actual tokens\n",
    "    pad_to_max_length=True,                          # Padding sequences to the same length\n",
    "    max_length=256,                                   # Maximum length of each sequence\n",
    "    return_tensors='pt'                               # Returning PyTorch tensors\n",
    ")\n",
    "\n",
    "# Encoding the text data in the validation set using batch_encode_plus\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].Text.values,              # Extracting text data for validation\n",
    "    add_special_tokens=True,                          # Adding special tokens like [CLS] and [SEP]\n",
    "    return_attention_mask=True,                      # Returning attention masks to focus on actual tokens\n",
    "    pad_to_max_length=True,                          # Padding sequences to the same length\n",
    "    max_length=256,                                   # Maximum length of each sequence\n",
    "    return_tensors='pt'                               # Returning PyTorch tensors\n",
    ")\n",
    "\n",
    "# Extracting input IDs, attention masks, and labels for the training set\n",
    "input_ids_train = encoded_data_train['input_ids']     # Input IDs representing tokenized text\n",
    "attention_masks_train = encoded_data_train['attention_mask']  # Attention masks indicating which tokens to attend to\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].Label.values)  # Labels for the training set\n",
    "\n",
    "# Extracting input IDs, attention masks, and labels for the validation set\n",
    "input_ids_val = encoded_data_val['input_ids']         # Input IDs representing tokenized text\n",
    "attention_masks_val = encoded_data_val['attention_mask']   # Attention masks indicating which tokens to attend to\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].Label.values)   # Labels for the validation set\n",
    "\n",
    "# Creating PyTorch datasets for training and validation\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)  # Training dataset\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)          # Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initializing the BERT model for sequence classification from the pre-trained 'bert-base-uncased' model\n",
    "# Specifying the number of labels in the output layer based on the length of the label dictionary\n",
    "# Setting output_attentions and output_hidden_states to False to exclude additional outputs\n",
    "# Setting resume_download to True to resume download if interrupted\n",
    "model = torch.load('../model/finetuned_BERT_epoch_2.model')\n",
    "\n",
    "# Defining the batch size for training and validation\n",
    "batch_size = 8\n",
    "\n",
    "# Creating data loaders for training and validation sets\n",
    "# Using RandomSampler for training data and SequentialSampler for validation data\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "# Initializing the AdamW optimizer with the BERT model parameters\n",
    "# Setting the learning rate to 2e-5 and epsilon to 1e-8\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5, \n",
    "                  eps=1e-8)\n",
    "\n",
    "# Defining the number of epochs for training\n",
    "epochs = 7\n",
    "\n",
    "# Creating a linear scheduler with warmup for adjusting learning rates during training\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "# Defining a function to calculate the F1 score\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "# Defining a function to calculate accuracy per class\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Training Loop\n",
    "Approach adapted from an older version of HuggingFace's run_glue.py script. Accessible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### assigning seed to be able to reproduce results ###\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Checking for GPU availability and assigning the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Moving the model to the selected device\n",
    "print(device)  # Printing the device (GPU or CPU) being used\n",
    "\n",
    "# Defining the evaluation function for the validation set\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()  # Setting the model to evaluation mode\n",
    "    \n",
    "    loss_val_total = 0  # Initializing total validation loss\n",
    "    predictions, true_vals = [], []  # Lists to store predictions and true values\n",
    "    \n",
    "    # Iterating through batches in the validation dataloader\n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)  # Moving batch tensors to the device\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],      # Input token IDs\n",
    "                  'attention_mask': batch[1],      # Attention masks\n",
    "                  'labels':         batch[2],      # Labels\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():  # Disabling gradient calculation        \n",
    "            outputs = model(**inputs)  # Forward pass\n",
    "            \n",
    "        loss = outputs[0]  # Extracting loss value from the output\n",
    "        logits = outputs[1]  # Predicted logits\n",
    "        loss_val_total += loss.item()  # Accumulating validation loss\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()  # Detaching logits from computation graph and moving to CPU\n",
    "        label_ids = inputs['labels'].cpu().numpy()  # Moving label IDs to CPU\n",
    "        predictions.append(logits)  # Appending predictions to the list\n",
    "        true_vals.append(label_ids)  # Appending true values to the list\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)  # Calculating average validation loss\n",
    "    \n",
    "    # Concatenating predictions and true values to form arrays\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals  # Returning validation loss, predictions, and true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060055aa0fb443fbb46d7aa05ebb4269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b281d64b064184b24bb71b4d880814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/11371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.5345066210420307\n",
      "Validation loss: 0.516738008986184\n",
      "F1 Score (Weighted): 0.7896856609244949\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec36740bef44aab876ff110e8303ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/11371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.46978826275587254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m loss_train_avg \u001b[38;5;241m=\u001b[39m loss_train_total\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader_train)  \u001b[38;5;66;03m# Calculating average training loss\u001b[39;00m\n\u001b[0;32m     44\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_train_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Printing training loss\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m val_loss, predictions, true_vals \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader_validation\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Evaluating on validation set\u001b[39;00m\n\u001b[0;32m     47\u001b[0m val_f1 \u001b[38;5;241m=\u001b[39m f1_score_func(predictions, true_vals)  \u001b[38;5;66;03m# Calculating F1 score\u001b[39;00m\n\u001b[0;32m     48\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Printing validation loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 25\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataloader_val)\u001b[0m\n\u001b[0;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m:      batch[\u001b[38;5;241m0\u001b[39m],      \u001b[38;5;66;03m# Input token IDs\u001b[39;00m\n\u001b[0;32m     20\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;241m1\u001b[39m],      \u001b[38;5;66;03m# Attention masks\u001b[39;00m\n\u001b[0;32m     21\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m:         batch[\u001b[38;5;241m2\u001b[39m],      \u001b[38;5;66;03m# Labels\u001b[39;00m\n\u001b[0;32m     22\u001b[0m          }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disabling gradient calculation        \u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extracting loss value from the output\u001b[39;00m\n\u001b[0;32m     28\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Predicted logits\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Big_Project\\Mashine-Learning---RoBerta---Base-Bert\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:367\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    363\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m    365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_probs, value_layer)\n\u001b[1;32m--> 367\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n\u001b[0;32m    369\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mview(new_context_layer_shape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop for each epoch\n",
    "cnt=0\n",
    "for epoch in tqdm(range(3, epochs+1)):\n",
    "    # Clear out accumulated gradients\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()  # Setting the model to training mode\n",
    "    \n",
    "    loss_train_total = 0  # Initializing total training loss\n",
    "\n",
    "    # Progress bar for training epoch\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()  # Resetting gradients\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)  # Moving batch tensors to the device\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],      # Input token IDs\n",
    "                  'attention_mask': batch[1],      # Attention masks\n",
    "                  'labels':         batch[2],      # Labels\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)  # Forward pass\n",
    "        \n",
    "        loss = outputs[0]  # Extracting loss value from the output\n",
    "        loss_train_total += loss.item()  # Accumulating training loss\n",
    "        loss.backward()  # Backpropagation\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clipping gradients to prevent explosion\n",
    "        \n",
    "        optimizer.step()  # Optimizer step\n",
    "        scheduler.step()  # Scheduler step\n",
    "        \n",
    "        # Updating progress bar with current training loss\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    # Saving model after each epoch\n",
    "    torch.save(model.state_dict(), f'../model/finetuned_BERT_epoch_{epoch}.model')  # Saving model after each epoch\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')  # Printing current epoch\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)  # Calculating average training loss\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')  # Printing training loss\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)  # Evaluating on validation set\n",
    "    val_f1 = f1_score_func(predictions, true_vals)  # Calculating F1 score\n",
    "    tqdm.write(f'Validation loss: {val_loss}')  # Printing validation loss\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')  # Printing F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Load mô hình từ tệp .model\n",
    "model = torch.load('../model/finetuned_BERT_epoch_2.model')\n",
    "\n",
    "# Tạo một mô hình BERT không có lớp phân loại\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Lấy đầu ra từ mô hình BERT\n",
    "last_hidden_state, pooled_output = bert(input_ids_train, attention_mask=attention_masks_train)\n",
    "\n",
    "# Tạo biểu đồ của mô hình BERT\n",
    "make_dot(pooled_output)\n",
    "\n",
    "# Tạo biểu đồ của mô hình BERT\n",
    "make_dot(last_hidden_state)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
